{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "\treturn any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notALink(str):\n",
    "\tlink = False\n",
    "\n",
    "\tif \".com\" not in str and \".org\" not in str:\n",
    "\t\tlink = True\n",
    "\tif \"http\" not in str:\n",
    "\t\tlink = link and True\n",
    "\n",
    "\treturn link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_english_dict(valid_data):\n",
    "\teng_dict = open(\"words_dictionary.json\", \"r\")\n",
    "\teng_dict_data = json.load(eng_dict)\n",
    "\teng_dict.close()\n",
    "\n",
    "\twords = valid_data\n",
    "\tremoved_words = []\n",
    "\n",
    "\tfor i in range(len(words)):\n",
    "\t\ttry:\n",
    "\t\t\tif eng_dict_data[words[i]] == 1:\n",
    "\t\t\t\twords.remove(words[i])\n",
    "# \t\t\t\tremoved_words.append(words[i])\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\trepr(e)\n",
    "\n",
    "\t# print removed_words\n",
    "\n",
    "\ttry:\n",
    "\t\twordsjson = open(\"removed_words.json\", \"r\")\n",
    "\t\trw_data = json.load(wordsjson)\n",
    "\t\twordsjson.close()\n",
    "\n",
    "\n",
    "\t\twordsjson = open(\"removed_words.json\", \"w\")\n",
    "\n",
    "\t\tfor removed_word in removed_words:\n",
    "\t\t\trw_data[removed_word] = 1\n",
    "\n",
    "\t\tjson.dump(rw_data, wordsjson)\n",
    "\t\twordsjson.close()\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint (repr(e))\n",
    "\n",
    "\treturn words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(all_lines):\n",
    "\tcount = 1\n",
    "\tcorpus_data = {}\n",
    "\tprev_string = \"\"\n",
    "\tvalid_data = []\n",
    "\tnew = True\n",
    "\tcurr_data = {}\n",
    "\tfor line in all_lines:\n",
    "\n",
    "\t\tcurr_line = line.split()\n",
    "\t\t# print curr_line, len(curr_line), curr_line[len(curr_line)-1]\n",
    "\n",
    "\t\tif count == 1:\n",
    "\t\t\tprint (line)\n",
    "\t\t\tcurr_data['title'] = line.upper()\n",
    "\n",
    "\t\telif count == 2: #- gets the year of the date\n",
    "\n",
    "\t\t\tfile_name = \"corpus/\"+curr_line[len(curr_line)-1]+\"-corpus.json\"\n",
    "\n",
    "\t\t\tif os.path.exists(file_name):\n",
    "\t\t\t\ttype_action = \"r\"\n",
    "\t\t\t\tnew = False\n",
    "\t\t\telse:\n",
    "\t\t\t\ttype_action = \"w\"\n",
    "\t\t\t\tnew = True\n",
    "\n",
    "\t\t\tprint (file_name, new, type_action)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tif not new:\n",
    "\t\t\t\tprint (\"not new: \" + file_name)\n",
    "\t\t\t\tcorpus = open(file_name)\n",
    "\t\t\t\tcorpus_data = json.load(corpus)\n",
    "\t\t\t\t# print corpus_data\n",
    "\t\t\t\t# print \"why\"\n",
    "\t\t\telse:\n",
    "\t\t\t\tcorpus = open(file_name, \"w\")\n",
    "\t\t\t\tcorpus_data = {}\n",
    "\t\t\t\tcorpus_data['corpus'] = []\n",
    "\t\t\t\t# json.dump(corpus_data-, corpus)\n",
    "\t\t\tcorpus.close()\n",
    "\n",
    "\t\t\tcurr_data['date'] = line\n",
    "\n",
    "\t\telif count == 3:\n",
    "\t\t\tcurr_data['author'] = line.upper()\n",
    "\n",
    "\t\telif count > 3: # start to tokenize at line 4 of the current file\n",
    "\n",
    "\t\t\twritten = False\n",
    "\t\t\ttitle = 0\n",
    "\t\t\tproper_noun = False\n",
    "\t\t\tif curr_line:\n",
    "\t\t\t\tfor i in range( len(curr_line) ):\n",
    "\t\t\t\t\tcurr_string = curr_line[i]\n",
    "\t\t\t\t\twritten = False\n",
    "\t\t\t\t\tproper_noun = False\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif curr_string.istitle(): # if the first letter of the word is uppercase\n",
    "\t\t\t\t\t\ttitle = 1\n",
    "\t\t\t\t\t\tif prev_string is not \"\": \t# if the word is not the very first word of the document\n",
    "\t\t\t\t\t\t\tlast_ch = prev_string[len(prev_string)-1]\n",
    "\t\t\t\t\t\t\tif last_ch != \".\" and last_ch != \")\": \t# checks if the previous string has a period .\n",
    "\t\t\t\t\t\t\t\tproper_noun = True\t\t\t\t\t# it's a proper noun if there is no . in the prev_str\n",
    "\n",
    "\t\t\t\t\t\tif not proper_noun:\n",
    "\n",
    "\t\t\t\t\t\t\tif not hasNumbers(curr_string) and notALink(curr_string):\t# check if string does not have any number or any \".com\" to signify a link\n",
    "\t\t\t\t\t\t\t\tcleaned_str = re.sub(r'[^A-Za-zÀ-ÿ.\\\\\\'-]', '', curr_string)\t# removes other characters that are not alphabet and selected symbols\n",
    "\t\t\t\t\t\t\t\tcleaned_str = cleaned_str.lower()\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\tif cleaned_str[len(cleaned_str)-1] == \".\":\n",
    "\t\t\t\t\t\t\t\t\tcleaned_str = cleaned_str[:-1]\n",
    "\n",
    "\t\t\t\t\t\t\t\t# if cleaned_str not in valid_data:\t# checks if current cleaned string is unique\n",
    "\t\t\t\t\t\t\t\tvalid_data.append(cleaned_str)\n",
    "\n",
    "\t\t\t\t\t\t\t\t# corpus.write(cleaned_str +\"\\n\")\n",
    "\t\t\t\t\t\t\t\twritten = True\n",
    "\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\twritten = False\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttitle = 2\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tif not hasNumbers(curr_string) and notALink(curr_string): # check if string does not have any number or any \".com\" to signify a link\n",
    "\t\t\t\t\t\t\tcleaned_str = re.sub(r'[^A-Za-z.\\\\\\'-]', '', curr_string)\n",
    "\t\t\t\t\t\t\tcleaned_str = cleaned_str.lower()\n",
    "\n",
    "\t\t\t\t\t\t\tif len(cleaned_str) > 0:\n",
    "\t\t\t\t\t\t\t\tif cleaned_str[len(cleaned_str)-1] == \".\":\n",
    "\t\t\t\t\t\t\t\t\tcleaned_str = cleaned_str[:-1]\n",
    "\n",
    "\t\t\t\t\t\t\t\t# json.dump(cleaned_str, corpus, ensure_ascii=False)\n",
    "\t\t\t\t\t\t\t\t# corpus.write(cleaned_str +\"\\n\")\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t# if cleaned_str not in valid_data: # checks if current cleaned string is unique\n",
    "\t\t\t\t\t\t\t\tvalid_data.append(cleaned_str)\n",
    "\n",
    "\t\t\t\t\t\t\t\twritten = True\n",
    "\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\twritten = False\n",
    "\n",
    "\t\t\t\t\t# if written:\n",
    "\t\t\t\t\t# \tprint curr_string, written, title, detect(curr_string.decode('utf-8').strip())\n",
    "\t\t\t\t\t# else:\n",
    "\t\t\t\t\t# \tprint curr_string, written, title\n",
    "\n",
    "\n",
    "\t\t\t\t\tprev_string = curr_string\n",
    "\n",
    "\t\tcount += 1\n",
    "\n",
    "\tvalid_data = check_english_dict(valid_data)\n",
    "\n",
    "\t# print valid_data, \"what\"\n",
    "\n",
    "\tcurr_data['words'] = valid_data\n",
    "\n",
    "\tcorpus_data['corpus'].append(curr_data)\n",
    "\tcorpus = open(file_name, \"w-\")\n",
    "\tjson.dump(corpus_data, corpus)\n",
    "\t# or\n",
    "\t# for vd in valid_data:\n",
    "\t# \tcorpus.write(vd+\"\\n\")\n",
    "\n",
    "\tcorpus.close()\n",
    "    \n",
    "\ttokenized_string = \"\"\n",
    "    \n",
    "\tfor datum in valid_data:\n",
    "\t\ttokenized_string = tokenized_string + datum + \" \"\n",
    "        \n",
    "\treturn tokenized_string\n",
    "\n",
    "\n",
    "\t# code for th-e unique corpus\n",
    "\t# if os.stat(UNIQUE_CORPUS).st_size > 2:\n",
    "\t# \tprint \"not empty\"\n",
    "\t# \twith open(UNIQUE_CORPUS, mode = \"r\") as uc:\n",
    "\t# \t\tuc_data = json.load(uc)\n",
    "\t# \tuc.close()\n",
    "\n",
    "\t# \twith open(UNIQUE_CORPUS, mode = \"w\") as uc:\n",
    "\t# \t\tfor curr_str in valid_data:\n",
    "\t# \t\t\tif curr_str not in uc_data:\n",
    "\t# \t\t\t\tuc_data.append(curr_str)\n",
    "\t# \t\tjson.dump(uc_data, uc)\n",
    "\n",
    "\t# \tuc.close()\n",
    "\t# else:\n",
    "\t# \twith open(UNIQUE_CORPUS, mode = \"w\") as uc:\n",
    "\t# \t\tjson.dump(valid_data, uc)\n",
    "\t# \tuc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
