{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, re\n",
    "texts = ['John likes to watch movies. Mary likes too.',\n",
    "   'John also likes to watch football games.']\n",
    "bagsofwords = [ collections.Counter(re.findall(r'\\w+', txt))\n",
    "            for txt in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John likes to watch movies. Mary likes too.',\n",
       " 'John also likes to watch football games.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'John': 1,\n",
       "          'Mary': 1,\n",
       "          'likes': 2,\n",
       "          'movies': 1,\n",
       "          'to': 1,\n",
       "          'too': 1,\n",
       "          'watch': 1}),\n",
       " Counter({'John': 1,\n",
       "          'also': 1,\n",
       "          'football': 1,\n",
       "          'games': 1,\n",
       "          'likes': 1,\n",
       "          'to': 1,\n",
       "          'watch': 1})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagsofwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus': [{'date': 'August 23, 2008', 'title': 'SA SURREALISM ROOM SA GUGGENHEIM COLLECTION', 'words': ['sa', 'surrealism', 'room', 'sa', 'guggenheim', 'collection', 'surrealism', 'room', 'mga', 'hulagway', 'giukay', 'mga', 'kutsilyo', 'ug', 'mga', 'bulok', 'mga', 'sukwahing', 'sa', 'mga', 'babayeng', 'ug', 'pak-an', 'siyudad', 'nga', 'nangatun-as', 'kabatoan', 'nasaag', 'ka', 'sa', 'mga', 'wala', 'bulok', 'nahipugwat', 'ka', 'sa', 'paghinumdom', 'dugay', 'mo', 'nang', 'nakalimtan', 'gihigot', 'ka', 'sa', 'kamingaw', 'mga', 'butang', 'nga', 'wala', 'pa', 'nimo', 'igkita', 'pa', 'matagamtam', 'bisan', 'kanus-a', 'ko', 'gyod', 'igkita', 'o', 'matilawan', 'ug', 'sa', 'tumang', 'kaseguro', 'nakaamgo', 'ka', 'ang', 'siyudad', 'naglutaw', 'sa', 'panganod', 'ang', 'tinuod', 'mong', 'gigikanan', 'sa', 'walay', 'pagduhaduha', 'walay', 'porma', 'ug', 'balikog', 'mga', 'puthaw', 'nga', 'nagpasad', 'sa', 'balas', 'buak', 'putol', 'nga', 'mga', 'bahin', 'tanghaga', 'nga', 'ang', 'tubag', 'ra', 'usab', 'ikaw', 'kining', 'balaka', 'napatik', 'usab', 'sa', 'gipatik', 'kini', 'uban', 'ang', 'pagtugot', 'sa', 'moderetor', 'nga', 'si'], 'author': 'ESTER TAPIA'}, {'date': 'March 24, 2008', 'author': '', 'words': ['tigi', 'sa', 'og', 'sa', 'ang', 'bulan', 'sa', 'maoy', 'of', 'the', 'sa', 'kini', 'ang', 'gideklarar', 'sa', 'no', 'sa', 'pagpakigduyog', 'ning', 'maong', 'okasyon', 'ang', 'ug', 'naglusad', 'og', 'tigi', 'sa', 'gumalaysay', 'personal', 'essay', 'mga', 'bukas', 'ang', 'tigi', 'sa', 'tanang', 'mga', 'batan-on', 'hangtod', 'sa', 'ang', 'edad', 'ang', 'salmot', 'kinahanglang', 'orihinal', 'ug', 'dili', 'hinubad', 'gikan', 'sa', 'laing', 'pinulongan', 'ug', 'dili', 'sinundog', 'gikan', 'sa', 'laing', 'mga', 'sinulat', 'ug', 'wala', 'pa', 'makadaog', 'og', 'major', 'prize', 'sa', 'ubang', 'kontes', 'ug', 'wala', 'pa', 'mapatik', 'sa', 'bisan', 'unsang', 'basahon', 'ug', 'sa', 'ang', 'mga', 'salmot', 'dili', 'butangan', 'og', 'timailhan', 'sa', 'tagsulat', 'bisag', 'dagangalan', 'isukip', 'sa', 'salmot', 'ang', 'pinirmahan', 'nga', 'nga', 'ang', 'salmot', 'iyang', 'kaugalingon', 'sa', 'tagsulat', 'ug', 'photocopy', 'sa', 'birth', 'certificate', 'isulod', 'kini', 'sa', 'lain', 'nga', 'sobre', 'uban', 'sa', 'sa', 'ug', 'ug', 'takpan', 'ang', 'gumalaysay', 'pinasikad', 'sa', 'tema', 'pag-amping', 'sa', 'sa', 'sa', 'duha', 'ngadto', 'sa', 'lima', 'ka', 'pahina', 'sa', 'double', 'space', 'sa', 'x', 'bond-paper', 'font', 'size', 'usa', 'lang', 'ka', 'entry', 'ang', 'ipadala', 'ang', 'mapatud-an', 'nga', 'nakalapas', 'sa', 'lagda', 'dili', 'na', 'mahiapil', 'sa', 'kontes', 'gikan', 'sa', 'mga', 'entry', 'magpili', 'og', 'napulo', 'ka', 'finalist', 'nga', 'ipatik', 'sa', 'ug', 'sugod', 'sa', 'ipahibalo', 'ang', 'mananaog', 'sa', 'mga', 'ug', 'sertipiko', 'ug', 'sertipiko', 'ug', 'sertipiko', 'ka', 'matag', 'usa', 'ug', 'sertipiko', 'dugang', 'niini', 'ang', 'mga', 'mananaog', 'nga', 'entry', 'dunay', 'kahigayonan', 'nga', 'mapatik', 'sa', 'magazine', 'ug', 'makadawat', 'usab', 'og', 'bayad', 'ang', 'desisyon', 'sa', 'mga', 'maghuhukom', 'ilhong', 'pinal', 'ug', 'dili', 'na', 'malubag', 'ang', 'pagpanawat', 'sa', 'salmot', 'kutob', 'lang', 'sa', 'ipadala', 'ang', 'salmot', 'sa', 'co', 'mb', 'corner', 'mahimo', 'usab', 'nga', 'ipadala', 'sa', 'e-mail', 'attachment', 'ngadto', 'sa', 'alang', 'sa', 'dugang', 'kasayoran', 'duawa', 'ang', 'o', 'o', 'di', 'ba', 'text', 'sa', 'numero'], 'title': 'TIGI SA SINULATAY OG GUMALAYSAY'}]}\n"
     ]
    }
   ],
   "source": [
    "data = {\"corpus\": [{\"date\": \"August 23, 2008\", \"title\": \"SA SURREALISM ROOM SA GUGGENHEIM COLLECTION\", \"words\": [\"sa\", \"surrealism\", \"room\", \"sa\", \"guggenheim\", \"collection\", \"surrealism\", \"room\", \"mga\", \"hulagway\", \"giukay\", \"mga\", \"kutsilyo\", \"ug\", \"mga\", \"bulok\", \"mga\", \"sukwahing\", \"sa\", \"mga\", \"babayeng\", \"ug\", \"pak-an\", \"siyudad\", \"nga\", \"nangatun-as\", \"kabatoan\", \"nasaag\", \"ka\", \"sa\", \"mga\", \"wala\", \"bulok\", \"nahipugwat\", \"ka\", \"sa\", \"paghinumdom\", \"dugay\", \"mo\", \"nang\", \"nakalimtan\", \"gihigot\", \"ka\", \"sa\", \"kamingaw\", \"mga\", \"butang\", \"nga\", \"wala\", \"pa\", \"nimo\", \"igkita\", \"pa\", \"matagamtam\", \"bisan\", \"kanus-a\", \"ko\", \"gyod\", \"igkita\", \"o\", \"matilawan\", \"ug\", \"sa\", \"tumang\", \"kaseguro\", \"nakaamgo\", \"ka\", \"ang\", \"siyudad\", \"naglutaw\", \"sa\", \"panganod\", \"ang\", \"tinuod\", \"mong\", \"gigikanan\", \"sa\", \"walay\", \"pagduhaduha\", \"walay\", \"porma\", \"ug\", \"balikog\", \"mga\", \"puthaw\", \"nga\", \"nagpasad\", \"sa\", \"balas\", \"buak\", \"putol\", \"nga\", \"mga\", \"bahin\", \"tanghaga\", \"nga\", \"ang\", \"tubag\", \"ra\", \"usab\", \"ikaw\", \"kining\", \"balaka\", \"napatik\", \"usab\", \"sa\", \"gipatik\", \"kini\", \"uban\", \"ang\", \"pagtugot\", \"sa\", \"moderetor\", \"nga\", \"si\"], \"author\": \"ESTER TAPIA\"}, {\"date\": \"March 24, 2008\", \"author\": \"\", \"words\": [\"tigi\", \"sa\", \"og\", \"sa\", \"ang\", \"bulan\", \"sa\", \"maoy\", \"of\", \"the\", \"sa\", \"kini\", \"ang\", \"gideklarar\", \"sa\", \"no\", \"sa\", \"pagpakigduyog\", \"ning\", \"maong\", \"okasyon\", \"ang\", \"ug\", \"naglusad\", \"og\", \"tigi\", \"sa\", \"gumalaysay\", \"personal\", \"essay\", \"mga\", \"bukas\", \"ang\", \"tigi\", \"sa\", \"tanang\", \"mga\", \"batan-on\", \"hangtod\", \"sa\", \"ang\", \"edad\", \"ang\", \"salmot\", \"kinahanglang\", \"orihinal\", \"ug\", \"dili\", \"hinubad\", \"gikan\", \"sa\", \"laing\", \"pinulongan\", \"ug\", \"dili\", \"sinundog\", \"gikan\", \"sa\", \"laing\", \"mga\", \"sinulat\", \"ug\", \"wala\", \"pa\", \"makadaog\", \"og\", \"major\", \"prize\", \"sa\", \"ubang\", \"kontes\", \"ug\", \"wala\", \"pa\", \"mapatik\", \"sa\", \"bisan\", \"unsang\", \"basahon\", \"ug\", \"sa\", \"ang\", \"mga\", \"salmot\", \"dili\", \"butangan\", \"og\", \"timailhan\", \"sa\", \"tagsulat\", \"bisag\", \"dagangalan\", \"isukip\", \"sa\", \"salmot\", \"ang\", \"pinirmahan\", \"nga\", \"nga\", \"ang\", \"salmot\", \"iyang\", \"kaugalingon\", \"sa\", \"tagsulat\", \"ug\", \"photocopy\", \"sa\", \"birth\", \"certificate\", \"isulod\", \"kini\", \"sa\", \"lain\", \"nga\", \"sobre\", \"uban\", \"sa\", \"sa\", \"ug\", \"ug\", \"takpan\", \"ang\", \"gumalaysay\", \"pinasikad\", \"sa\", \"tema\", \"pag-amping\", \"sa\", \"sa\", \"sa\", \"duha\", \"ngadto\", \"sa\", \"lima\", \"ka\", \"pahina\", \"sa\", \"double\", \"space\", \"sa\", \"x\", \"bond-paper\", \"font\", \"size\", \"usa\", \"lang\", \"ka\", \"entry\", \"ang\", \"ipadala\", \"ang\", \"mapatud-an\", \"nga\", \"nakalapas\", \"sa\", \"lagda\", \"dili\", \"na\", \"mahiapil\", \"sa\", \"kontes\", \"gikan\", \"sa\", \"mga\", \"entry\", \"magpili\", \"og\", \"napulo\", \"ka\", \"finalist\", \"nga\", \"ipatik\", \"sa\", \"ug\", \"sugod\", \"sa\", \"ipahibalo\", \"ang\", \"mananaog\", \"sa\", \"mga\", \"ug\", \"sertipiko\", \"ug\", \"sertipiko\", \"ug\", \"sertipiko\", \"ka\", \"matag\", \"usa\", \"ug\", \"sertipiko\", \"dugang\", \"niini\", \"ang\", \"mga\", \"mananaog\", \"nga\", \"entry\", \"dunay\", \"kahigayonan\", \"nga\", \"mapatik\", \"sa\", \"magazine\", \"ug\", \"makadawat\", \"usab\", \"og\", \"bayad\", \"ang\", \"desisyon\", \"sa\", \"mga\", \"maghuhukom\", \"ilhong\", \"pinal\", \"ug\", \"dili\", \"na\", \"malubag\", \"ang\", \"pagpanawat\", \"sa\", \"salmot\", \"kutob\", \"lang\", \"sa\", \"ipadala\", \"ang\", \"salmot\", \"sa\", \"co\", \"mb\", \"corner\", \"mahimo\", \"usab\", \"nga\", \"ipadala\", \"sa\", \"e-mail\", \"attachment\", \"ngadto\", \"sa\", \"alang\", \"sa\", \"dugang\", \"kasayoran\", \"duawa\", \"ang\", \"o\", \"o\", \"di\", \"ba\", \"text\", \"sa\", \"numero\"], \"title\": \"TIGI SA SINULATAY OG GUMALAYSAY\"}]}\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_english_dict(valid_data):\n",
    "\teng_dict = open(\"words_dictionary.json\", \"r\")\n",
    "\teng_dict_data = json.load(eng_dict)\n",
    "\teng_dict.close()\n",
    "\n",
    "\twords = valid_data\n",
    "\tremoved_words = []\n",
    "\n",
    "\tfor i in range(len(words)):\n",
    "\t\ttry:\n",
    "\t\t\tif eng_dict_data[words[i]] == 1:\n",
    "\t\t\t\t# words.remove(words[i])\n",
    "\t\t\t\tremoved_words.append(words[i])\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\trepr(e)\n",
    "\n",
    "\t# print removed_words\n",
    "\n",
    "\ttry:\n",
    "\t\twordsjson = open(\"removed_words.json\", \"r\")\n",
    "\t\trw_data = json.load(wordsjson)\n",
    "\t\twordsjson.close()\n",
    "\n",
    "\n",
    "\t\twordsjson = open(\"removed_words.json\", \"w\")\n",
    "\n",
    "\t\tfor removed_word in removed_words:\n",
    "\t\t\trw_data[removed_word] = 1\n",
    "\n",
    "\t\tjson.dump(rw_data, wordsjson)\n",
    "\t\twordsjson.close()\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint (repr(e))\n",
    "\n",
    "\treturn words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(nt line)? (<ipython-input-2-d51031968aba>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d51031968aba>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    print line\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(nt line)?\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(all_lines):\n",
    "\tcount = 1\n",
    "\tcorpus_data = {}\n",
    "\tprev_string = \"\"\n",
    "\tvalid_data = []\n",
    "\tnew = True\n",
    "\tcurr_data = {}\n",
    "\tfor line in all_lines:\n",
    "\n",
    "\t\tcurr_line = line.split()\n",
    "\t\t# print curr_line, len(curr_line), curr_line[len(curr_line)-1]\n",
    "\n",
    "\t\tif count == 1:\n",
    "\t\t\tprint line\n",
    "\t\t\tcurr_data['title'] = line.upper()\n",
    "\n",
    "\t\telif count == 2: #- gets the year of the date\n",
    "\n",
    "\t\t\tfile_name = \"corpus/\"+curr_line[len(curr_line)-1]+\"-corpus.json\"\n",
    "\n",
    "\t\t\tif os.path.exists(file_name):\n",
    "\t\t\t\ttype_action = \"r\"\n",
    "\t\t\t\tnew = False\n",
    "\t\t\telse:\n",
    "\t\t\t\ttype_action = \"w\"\n",
    "\t\t\t\tnew = True\n",
    "\n",
    "\t\t\tprint file_name, new, type_action\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tif not new:\n",
    "\t\t\t\tprint \"not new: \" + file_name\n",
    "\t\t\t\tcorpus = open(file_name)\n",
    "\t\t\t\tcorpus_data = json.load(corpus)\n",
    "\t\t\t\t# print corpus_data\n",
    "\t\t\t\t# print \"why\"\n",
    "\t\t\telse:\n",
    "\t\t\t\tcorpus = open(file_name, \"w\")\n",
    "\t\t\t\tcorpus_data = {}\n",
    "\t\t\t\tcorpus_data['corpus'] = []\n",
    "\t\t\t\t# json.dump(corpus_data-, corpus)\n",
    "\t\t\tcorpus.close()\n",
    "\n",
    "\t\t\tcurr_data['date'] = line\n",
    "\n",
    "\t\telif count == 3:\n",
    "\t\t\tcurr_data['author'] = line.upper()\n",
    "\n",
    "\t\telif count > 3: # start to tokenize at line 4 of the current file\n",
    "\n",
    "\t\t\twritten = False\n",
    "\t\t\ttitle = 0\n",
    "\t\t\tproper_noun = False\n",
    "\t\t\tif curr_line:\n",
    "\t\t\t\tfor i in range( len(curr_line) ):\n",
    "\t\t\t\t\tcurr_string = curr_line[i]\n",
    "\t\t\t\t\twritten = False\n",
    "\t\t\t\t\tproper_noun = False\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif curr_string.istitle(): # if the first letter of the word is uppercase\n",
    "\t\t\t\t\t\ttitle = 1\n",
    "\t\t\t\t\t\tif prev_string is not \"\": \t# if the word is not the very first word of the document\n",
    "\t\t\t\t\t\t\tlast_ch = prev_string[len(prev_string)-1]\n",
    "\t\t\t\t\t\t\tif last_ch != \".\" and last_ch != \")\": \t# checks if the previous string has a period .\n",
    "\t\t\t\t\t\t\t\tproper_noun = True\t\t\t\t\t# it's a proper noun if there is no . in the prev_str\n",
    "\n",
    "\t\t\t\t\t\tif not proper_noun:\n",
    "\n",
    "\t\t\t\t\t\t\tif not hasNumbers(curr_string) and notALink(curr_string):\t# check if string does not have any number or any \".com\" to signify a link\n",
    "\t\t\t\t\t\t\t\tcleaned_str = re.sub(r'[^A-Za-zÀ-ÿ.\\\\\\'-]', '', curr_string)\t# removes other characters that are not alphabet and selected symbols\n",
    "\t\t\t\t\t\t\t\tcleaned_str = cleaned_str.lower()\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\tif cleaned_str[len(cleaned_str)-1] == \".\":\n",
    "\t\t\t\t\t\t\t\t\tcleaned_str = cleaned_str[:-1]\n",
    "\n",
    "\t\t\t\t\t\t\t\t# if cleaned_str not in valid_data:\t# checks if current cleaned string is unique\n",
    "\t\t\t\t\t\t\t\tvalid_data.append(cleaned_str)\n",
    "\n",
    "\t\t\t\t\t\t\t\t# corpus.write(cleaned_str +\"\\n\")\n",
    "\t\t\t\t\t\t\t\twritten = True\n",
    "\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\twritten = False\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttitle = 2\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tif not hasNumbers(curr_string) and notALink(curr_string): # check if string does not have any number or any \".com\" to signify a link\n",
    "\t\t\t\t\t\t\tcleaned_str = re.sub(r'[^A-Za-z.\\\\\\'-]', '', curr_string)\n",
    "\t\t\t\t\t\t\tcleaned_str = cleaned_str.lower()\n",
    "\n",
    "\t\t\t\t\t\t\tif len(cleaned_str) > 0:\n",
    "\t\t\t\t\t\t\t\tif cleaned_str[len(cleaned_str)-1] == \".\":\n",
    "\t\t\t\t\t\t\t\t\tcleaned_str = cleaned_str[:-1]\n",
    "\n",
    "\t\t\t\t\t\t\t\t# json.dump(cleaned_str, corpus, ensure_ascii=False)\n",
    "\t\t\t\t\t\t\t\t# corpus.write(cleaned_str +\"\\n\")\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t# if cleaned_str not in valid_data: # checks if current cleaned string is unique\n",
    "\t\t\t\t\t\t\t\tvalid_data.append(cleaned_str)\n",
    "\n",
    "\t\t\t\t\t\t\t\twritten = True\n",
    "\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\twritten = False\n",
    "\n",
    "\t\t\t\t\t# if written:\n",
    "\t\t\t\t\t# \tprint curr_string, written, title, detect(curr_string.decode('utf-8').strip())\n",
    "\t\t\t\t\t# else:\n",
    "\t\t\t\t\t# \tprint curr_string, written, title\n",
    "\n",
    "\n",
    "\t\t\t\t\tprev_string = curr_string\n",
    "\n",
    "\t\tcount += 1\n",
    "\n",
    "\tvalid_data = check_english_dict(valid_data)\n",
    "\n",
    "\t# print valid_data, \"what\"\n",
    "\n",
    "\tcurr_data['words'] = valid_data\n",
    "\n",
    "\tcorpus_data['corpus'].append(curr_data)\n",
    "\tcorpus = open(file_name, \"w-\")\n",
    "\tjson.dump(corpus_data, corpus)\n",
    "\t# or\n",
    "\t# for vd in valid_data:\n",
    "\t# \tcorpus.write(vd+\"\\n\")\n",
    "\n",
    "\tcorpus.close()\n",
    "\n",
    "\n",
    "\t# code for th-e unique corpus\n",
    "\t# if os.stat(UNIQUE_CORPUS).st_size > 2:\n",
    "\t# \tprint \"not empty\"\n",
    "\t# \twith open(UNIQUE_CORPUS, mode = \"r\") as uc:\n",
    "\t# \t\tuc_data = json.load(uc)\n",
    "\t# \tuc.close()\n",
    "\n",
    "\t# \twith open(UNIQUE_CORPUS, mode = \"w\") as uc:\n",
    "\t# \t\tfor curr_str in valid_data:\n",
    "\t# \t\t\tif curr_str not in uc_data:\n",
    "\t# \t\t\t\tuc_data.append(curr_str)\n",
    "\t# \t\tjson.dump(uc_data, uc)\n",
    "\n",
    "\t# \tuc.close()\n",
    "\t# else:\n",
    "\t# \twith open(UNIQUE_CORPUS, mode = \"w\") as uc:\n",
    "\t# \t\tjson.dump(valid_data, uc)\n",
    "\t# \tuc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 2 1 2 1 1 1]\n",
      " [1 1 1 1 1 0 0 1 0 1]]\n",
      "['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-0e441b243c99>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-0e441b243c99>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tryyyy = [collections.Counter(re.findall(r'\\w+', txt)) words for cor in data['corpus']]\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tryyyy = [collections.Counter(re.findall(r'\\w+', txt)) words for cor in data['corpus']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  1  4  1  0  0  1  1  1  1  1  0  0  0  0  0  1  0  1  0  0  2  1\n",
      "   0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  1  0\n",
      "   1  1  1  0  1  0  0  1  2  1  0  0  0  0  0  0  0  4  1  0  1  1  0  1\n",
      "   0  0  1  1  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  1  1  0  9  1  1  1  0  0  1  1  1  1  0  1  1  1  1  0\n",
      "   1  6  0  0  1  0  0  0  0  0  0  0  0  2  0  1  1  0  0  1  0  1  1  0\n",
      "   0  0  0  0  0  0  1  0  1  1  1  2 12  0  0  1  0  0  2  0  0  0  0  1\n",
      "   2  0  0  0  1  0  0  0  0  0  1  1  1  1  0  4  0  0  2  2  2]\n",
      " [ 1  1  1 18  0  1  1  0  0  0  0  0  1  1  1  1  1  1  1  0  1  1  0  0\n",
      "   1  1  1  0  1  1  1  1  5  1  1  2  0  1  1  1  3  1  1  1  1  0  0  3\n",
      "   0  0  0  2  0  1  1  0  0  0  1  3  1  1  1  1  1  4  0  1  0  0  1  0\n",
      "   1  1  2  0  0  2  1  0  1  1  2  2  1  1  1  1  1  1  1  1  1  1  1  2\n",
      "   1  1  2  1  1  0  0  1  8  0  0  0  2  1  0  0  0  0  1  0  0  0  0  1\n",
      "   0  8  2  1  0  1  1  1  1  6  1  1  1  2  1  0  0  1  1  0  1  0  0  1\n",
      "   1  1  1  1  1  1  0  1  0  0  0  0 43  6  4  0  1  1  0  1  1  1  1  0\n",
      "   0  2  1  1  0  1  1  1  3  1  0  0  0  1  1 16  1  2  2  2  0]]\n",
      "['alang', 'amping', 'an', 'ang', 'as', 'attachment', 'ba', 'babayeng', 'bahin', 'balaka', 'balas', 'balikog', 'basahon', 'batan', 'bayad', 'birth', 'bisag', 'bisan', 'bond', 'buak', 'bukas', 'bulan', 'bulok', 'butang', 'butangan', 'certificate', 'co', 'collection', 'corner', 'dagangalan', 'desisyon', 'di', 'dili', 'double', 'duawa', 'dugang', 'dugay', 'duha', 'dunay', 'edad', 'entry', 'essay', 'finalist', 'font', 'gideklarar', 'gigikanan', 'gihigot', 'gikan', 'gipatik', 'giukay', 'guggenheim', 'gumalaysay', 'gyod', 'hangtod', 'hinubad', 'hulagway', 'igkita', 'ikaw', 'ilhong', 'ipadala', 'ipahibalo', 'ipatik', 'isukip', 'isulod', 'iyang', 'ka', 'kabatoan', 'kahigayonan', 'kamingaw', 'kanus', 'kasayoran', 'kaseguro', 'kaugalingon', 'kinahanglang', 'kini', 'kining', 'ko', 'kontes', 'kutob', 'kutsilyo', 'lagda', 'lain', 'laing', 'lang', 'lima', 'magazine', 'maghuhukom', 'magpili', 'mahiapil', 'mahimo', 'mail', 'major', 'makadaog', 'makadawat', 'malubag', 'mananaog', 'maong', 'maoy', 'mapatik', 'mapatud', 'matag', 'matagamtam', 'matilawan', 'mb', 'mga', 'mo', 'moderetor', 'mong', 'na', 'naglusad', 'naglutaw', 'nagpasad', 'nahipugwat', 'nakaamgo', 'nakalapas', 'nakalimtan', 'nang', 'nangatun', 'napatik', 'napulo', 'nasaag', 'nga', 'ngadto', 'niini', 'nimo', 'ning', 'no', 'numero', 'of', 'og', 'okasyon', 'on', 'orihinal', 'pa', 'pag', 'pagduhaduha', 'paghinumdom', 'pagpakigduyog', 'pagpanawat', 'pagtugot', 'pahina', 'pak', 'panganod', 'paper', 'personal', 'photocopy', 'pinal', 'pinasikad', 'pinirmahan', 'pinulongan', 'porma', 'prize', 'puthaw', 'putol', 'ra', 'room', 'sa', 'salmot', 'sertipiko', 'si', 'sinulat', 'sinundog', 'siyudad', 'size', 'sobre', 'space', 'sugod', 'sukwahing', 'surrealism', 'tagsulat', 'takpan', 'tanang', 'tanghaga', 'tema', 'text', 'the', 'tigi', 'timailhan', 'tinuod', 'tubag', 'tumang', 'uban', 'ubang', 'ug', 'unsang', 'usa', 'usab', 'wala', 'walay']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "vocabulary not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7bfaf0843c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# feature_names.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mbog_docu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: vocabulary not found"
     ]
    }
   ],
   "source": [
    "docu_level = []\n",
    "\n",
    "for cor in data['corpus']:\n",
    "    ind_words = cor['words']\n",
    "    docu_string = \"\"\n",
    "    for ind_word in ind_words:\n",
    "        docu_string = docu_string + ind_word + \" \"\n",
    "        \n",
    "    docu_level.append(docu_string)\n",
    "    \n",
    "# print (docu_level)\n",
    "\n",
    "bog_docu = vectorizer.fit_transform(docu_level) \n",
    "\n",
    "print(bog_docu.toarray())\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# bog = open(\"bog.json\", \"w\")\n",
    "# json.dump(bog_docu.toarray(), bog)\n",
    "# bog.close()\n",
    "\n",
    "# feature_names = open(\"feature_names.json\", \"w\")\n",
    "# json.dump(vectorizer.get_feature_names(), feature_names)\n",
    "# feature_names.close()\n",
    "\n",
    "bog_docu.vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 159)\t1\n",
      "  (0, 106)\t1\n",
      "  (0, 139)\t1\n",
      "  (0, 181)\t1\n",
      "  (0, 74)\t1\n",
      "  (0, 48)\t1\n",
      "  (0, 118)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 57)\t1\n",
      "  (0, 186)\t2\n",
      "  (0, 154)\t1\n",
      "  (0, 179)\t1\n",
      "  (0, 172)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 153)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 111)\t1\n",
      "  (0, 152)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 150)\t1\n",
      "  (0, 135)\t1\n",
      "  (0, 188)\t2\n",
      "  (0, 45)\t1\n",
      "  :\t:\n",
      "  (1, 130)\t1\n",
      "  (1, 96)\t1\n",
      "  (1, 125)\t1\n",
      "  (1, 137)\t1\n",
      "  (1, 126)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 175)\t1\n",
      "  (1, 128)\t1\n",
      "  (1, 97)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 129)\t6\n",
      "  (1, 176)\t3\n",
      "  (1, 181)\t1\n",
      "  (1, 74)\t2\n",
      "  (1, 186)\t2\n",
      "  (1, 3)\t18\n",
      "  (1, 17)\t1\n",
      "  (1, 133)\t2\n",
      "  (1, 187)\t2\n",
      "  (1, 65)\t4\n",
      "  (1, 121)\t8\n",
      "  (1, 2)\t1\n",
      "  (1, 183)\t16\n",
      "  (1, 104)\t8\n",
      "  (1, 156)\t43\n",
      "0 alang\n",
      "1 amping\n",
      "2 an\n",
      "3 ang\n",
      "4 as\n",
      "5 attachment\n",
      "6 ba\n",
      "7 babayeng\n",
      "8 bahin\n",
      "9 balaka\n",
      "10 balas\n",
      "11 balikog\n",
      "12 basahon\n",
      "13 batan\n",
      "14 bayad\n",
      "15 birth\n",
      "16 bisag\n",
      "17 bisan\n",
      "18 bond\n",
      "19 buak\n",
      "20 bukas\n",
      "21 bulan\n",
      "22 bulok\n",
      "23 butang\n",
      "24 butangan\n",
      "25 certificate\n",
      "26 co\n",
      "27 collection\n",
      "28 corner\n",
      "29 dagangalan\n",
      "30 desisyon\n",
      "31 di\n",
      "32 dili\n",
      "33 double\n",
      "34 duawa\n",
      "35 dugang\n",
      "36 dugay\n",
      "37 duha\n",
      "38 dunay\n",
      "39 edad\n",
      "40 entry\n",
      "41 essay\n",
      "42 finalist\n",
      "43 font\n",
      "44 gideklarar\n",
      "45 gigikanan\n",
      "46 gihigot\n",
      "47 gikan\n",
      "48 gipatik\n",
      "49 giukay\n",
      "50 guggenheim\n",
      "51 gumalaysay\n",
      "52 gyod\n",
      "53 hangtod\n",
      "54 hinubad\n",
      "55 hulagway\n",
      "56 igkita\n",
      "57 ikaw\n",
      "58 ilhong\n",
      "59 ipadala\n",
      "60 ipahibalo\n",
      "61 ipatik\n",
      "62 isukip\n",
      "63 isulod\n",
      "64 iyang\n",
      "65 ka\n",
      "66 kabatoan\n",
      "67 kahigayonan\n",
      "68 kamingaw\n",
      "69 kanus\n",
      "70 kasayoran\n",
      "71 kaseguro\n",
      "72 kaugalingon\n",
      "73 kinahanglang\n",
      "74 kini\n",
      "75 kining\n",
      "76 ko\n",
      "77 kontes\n",
      "78 kutob\n",
      "79 kutsilyo\n",
      "80 lagda\n",
      "81 lain\n",
      "82 laing\n",
      "83 lang\n",
      "84 lima\n",
      "85 magazine\n",
      "86 maghuhukom\n",
      "87 magpili\n",
      "88 mahiapil\n",
      "89 mahimo\n",
      "90 mail\n",
      "91 major\n",
      "92 makadaog\n",
      "93 makadawat\n",
      "94 malubag\n",
      "95 mananaog\n",
      "96 maong\n",
      "97 maoy\n",
      "98 mapatik\n",
      "99 mapatud\n",
      "100 matag\n",
      "101 matagamtam\n",
      "102 matilawan\n",
      "103 mb\n",
      "104 mga\n",
      "105 mo\n",
      "106 moderetor\n",
      "107 mong\n",
      "108 na\n",
      "109 naglusad\n",
      "110 naglutaw\n",
      "111 nagpasad\n",
      "112 nahipugwat\n",
      "113 nakaamgo\n",
      "114 nakalapas\n",
      "115 nakalimtan\n",
      "116 nang\n",
      "117 nangatun\n",
      "118 napatik\n",
      "119 napulo\n",
      "120 nasaag\n",
      "121 nga\n",
      "122 ngadto\n",
      "123 niini\n",
      "124 nimo\n",
      "125 ning\n",
      "126 no\n",
      "127 numero\n",
      "128 of\n",
      "129 og\n",
      "130 okasyon\n",
      "131 on\n",
      "132 orihinal\n",
      "133 pa\n",
      "134 pag\n",
      "135 pagduhaduha\n",
      "136 paghinumdom\n",
      "137 pagpakigduyog\n",
      "138 pagpanawat\n",
      "139 pagtugot\n",
      "140 pahina\n",
      "141 pak\n",
      "142 panganod\n",
      "143 paper\n",
      "144 personal\n",
      "145 photocopy\n",
      "146 pinal\n",
      "147 pinasikad\n",
      "148 pinirmahan\n",
      "149 pinulongan\n",
      "150 porma\n",
      "151 prize\n",
      "152 puthaw\n",
      "153 putol\n",
      "154 ra\n",
      "155 room\n",
      "156 sa\n",
      "157 salmot\n",
      "158 sertipiko\n",
      "159 si\n",
      "160 sinulat\n",
      "161 sinundog\n",
      "162 siyudad\n",
      "163 size\n",
      "164 sobre\n",
      "165 space\n",
      "166 sugod\n",
      "167 sukwahing\n",
      "168 surrealism\n",
      "169 tagsulat\n",
      "170 takpan\n",
      "171 tanang\n",
      "172 tanghaga\n",
      "173 tema\n",
      "174 text\n",
      "175 the\n",
      "176 tigi\n",
      "177 timailhan\n",
      "178 tinuod\n",
      "179 tubag\n",
      "180 tumang\n",
      "181 uban\n",
      "182 ubang\n",
      "183 ug\n",
      "184 unsang\n",
      "185 usa\n",
      "186 usab\n",
      "187 wala\n",
      "188 walay\n"
     ]
    }
   ],
   "source": [
    "print (bog_docu)\n",
    "names = vectorizer.get_feature_names()\n",
    "\n",
    "count = 0\n",
    "for name in names:\n",
    "    print (count, name)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
